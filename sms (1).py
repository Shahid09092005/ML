# -*- coding: utf-8 -*-
"""sms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SRj1J6zmb9NjfRYhpn8t4mNgawKO5qOq

**Setting Kaggle part**
"""

!pip install -q kaggle
from google.colab import files
files.upload()

!mkdir ~/.kaggle

!cp /content/kaggle.json ~/.kaggle
#permisson for the json to act
! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d uciml/sms-spam-collection-dataset

!unzip /content/sms-spam-collection-dataset.zip

"""**Cleaning part**"""

import pandas as pd
folder = pd.read_csv('/content/spam.csv', encoding='latin1')

folder.head()

folder.shape

folder.info()

folder.head()

folder.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)
#permanently deleted the last 3 colums with their name

folder.rename(columns={'v1':'Check','v2':'information'},inplace=True)

folder.sample(4)

folder.isnull().sum()

#here importing the sklearn.preprocessing and label the values of check
from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()

# 0 is assign for not-spam or ham and 1 is assign for spam
folder['Check']=encoder.fit_transform(folder['Check'])

folder.sample(4)

#check for duplicates
folder.duplicated().sum()

folder.shape

#droping duplicates
folder.drop_duplicates(keep='first',inplace=True)

folder.shape  #before removing duplicates value there is (5512)

"""**EDA(Exploratory Data Analysis) part**"""

#counting the total spam and not spam value by suing check colum
folder['Check'].value_counts()

#ploting a pic char between 0 and 1
import matplotlib.pyplot as plt
plt.pie(folder['check'].value_counts(),labels=['not-spam','spam'],autopct="%0.3f")
plt.show()

"""data is imbalanced because spam and not-spam sms are only 13% and 87% rep.

For Deeper analyse we add 3 different colums
"""

import nltk
from nltk.tokenize import word_tokenize,sent_tokenize

import nltk
nltk.download('punkt')

folder['num_chars']=folder['information'].apply(len)

folder['num_words']=folder['information'].apply(lambda x:len(word_tokenize(x)))

folder['num_sent']=folder['information'].apply(lambda x:len(sent_tokenize(x)))

folder.sample(5)

folder[['num_chars','num_words','num_sent']].describe()

#not spam
folder[folder['Check']==0][['num_chars','num_words','num_sent']].describe()

#for spam sms
folder[folder['Check']==1][['num_chars','num_words','num_sent']].describe()

import seaborn as sns
sns.histplot(folder[folder['Check']==0]['num_chars'])
sns.histplot(folder[folder['Check']==1]['num_chars'],color='red')

sns.histplot(folder[folder['Check']==0]['num_words'])
sns.histplot(folder[folder['Check']==1]['num_words'],color='purple')

#relation between non-spam(0) and spam(1)
sns.pairplot(folder,hue='Check')

folder.dtypes

"""in this we firstly drop the information colums then we perform this"""

temp_folder=folder[['Check','num_chars','num_words','num_sent']]

sns.heatmap(temp_folder.corr(),annot=True)

folder.sample(5)

"""**Data Preprocessing**"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords #stopwords are that have  no meaning but they help in the formation of the sentences
stopwords.words('english')

import string
string.punctuation #these are the punctuation

from nltk.stem.porter import PorterStemmer    #Doing Steamming
ps=PorterStemmer()
ps.stem('Loved')

def transform_text(text):
  text=text.lower() #converting into lower case
  text=nltk.word_tokenize(text) #Tokenization on the behalf of words using nltk

  y=[]
  for i in text: #removing special character(like !,@<,$,#,%,&,* etc)
    if i.isalnum():
      y.append(i)
  text=y[:]
  y.clear()

  for i in text:
    if i not in string.punctuation and i not in stopwords.words('english'):
      y.append(i)

  text=y[:]
  y.clear()

  for i in text:    # Steamming part
    y.append(ps.stem(i))
  text.clear()
  # return y # this returning the in the form of list
  return " ".join(y) # This is returning by making string

transform_text("I LOVED THE yt lectures on Machine Learning.? and How about you?")

transform_text(folder['information'][0])

folder['transform_text']=folder['information'].apply(transform_text) #making a seperate column of transform_text

folder.sample(3)

"""**Words Cloud for spam and non-spam**"""

!pip install wordcloud

from wordcloud import WordCloud
wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')

# for spam
import matplotlib.pyplot as plt
spam_wc=wc.generate(folder[folder['Check']==1]['transform_text'].str.cat(sep=" "))
plt.imshow(spam_wc)

#for non-spam
spam_wc=wc.generate(folder[folder['Check']==0]['transform_text'].str.cat(sep=" "))
plt.figure(figsize=(15,6))
plt.imshow(spam_wc)

"""now we are extracting the top 30 words that uses in spam and non-spam"""

# for SPAM
split_word=[]
for msg in folder[folder['Check']==1]['transform_text'].tolist(): # it convert the spam text of transform_text into list
  for word in msg.split():  #here each msg is splilting .
    split_word.append(word)
    # print(split_word)

split_word

len(split_word)

#  It will create a dictonary of repeating numbers of word that present in it
from collections import Counter
common=Counter(split_word).most_common(30) # this will give the most common occuring 30 words in the spam msg

df_common=pd.DataFrame(common) #df_common is a dataframe of the most 30 common words

"""**NOTE: sns.barplot() expects either a list, tuple, or pandas Series for both x and y parameters.**"""

#  df_common.iloc[:, column_name]  # Accessing the first column by index
#  df_common.iloc[:, Column_name]  # Accessing the second column by index

sns.barplot(x=df_common.iloc[:,0],y=df_common.iloc[:,1]) # here we are drawing a barplot between the common word dataframe between it's 0 and 1 column
plt.xticks(rotation='vertical')
plt.show()

# for NON-SPAM(0)
split_word=[]
for msg in folder[folder['Check']==0]['transform_text'].tolist(): # it convert the spam text of transform_text into list
  for word in msg.split():  #here each msg is splilting .
    split_word.append(word)

#  It will create a dictonary of repeating numbers of word that present in it
from collections import Counter
common=Counter(split_word).most_common(30) # this will give the most common occuring 30 words in the nnon-spam msg

df_common=pd.DataFrame(common) #df_common is a dataframe of the most 30 common words

#  df_common.iloc[:, column_name]  # Accessing the first column by index
#  df_common.iloc[:, Column_name]  # Accessing the second column by index

# here we are drawing a barplot between the common word dataframe between it's 0 and 1 column
plt.xticks(rotation='vertical')
sns.barplot(x=df_common.iloc[:,0],y=df_common.iloc[:,1],palette='Set3')  # palette='Set3' it will add different colors on the columns of bar graph
plt.xticks(rotation='vertical')
plt.xlabel('Word')
plt.ylabel('Number')
plt.show()

"""**Model Building**"""

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv=CountVectorizer()
tfidf=TfidfVectorizer(max_features=3000)

x=tfidf.fit_transform(folder['transform_text']).toarray()

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
x=scaler.fit_transform(x)

x.shape

y=folder['Check'].values

y

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()

gnb.fit(x_train,y_train)
y_pred1=gnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(x_train,y_train)
y_pred1=mnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

bnb.fit(x_train,y_train)
y_pred1=bnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

"""**tfidf--->MNB**"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

svc=SVC(kernel='sigmoid',gamma=1.0)
knc=KNeighborsClassifier()
mnb=MultinomialNB()
dtc=DecisionTreeClassifier(max_depth=5)
lrc=LogisticRegression(solver='liblinear',penalty='l1')
rfc=RandomForestClassifier(n_estimators=50,random_state=2)
abc=AdaBoostClassifier(n_estimators=50,random_state=2)
bc=BaggingClassifier(n_estimators=50,random_state=2)
etc=ExtraTreesClassifier(n_estimators=50,random_state=2)
gbdt=GradientBoostingClassifier(n_estimators=50,random_state=2)
xgb=XGBClassifier(n_estimators=50,random_state=2)

from sklearn.metrics import accuracy_score, precision_score

clfs = {
    'SVC': svc,
    'KN': knc,
    'NB': mnb,
    'DT': dtc,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'Bgc': bc,
    'ETC': etc,
    'GBDT': gbdt,
    'xgb': xgb
}

def train_classifier(clf, x_train, y_train, x_test, y_test):
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    return accuracy, precision

train_classifier(svc,x_train,y_train,x_test,y_test)

accuracy_scores = []
precision_scores = []

for name, clf in clfs.items():
    current_accuracy, current_precision = train_classifier(clf, x_train, y_train, x_test, y_test)
    print("For ", name)
    print("Accuracy - ", current_accuracy)
    print("Precision - ", current_precision)

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)

performance_df=pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)

performance_df

performance_df1=pd.melt(performance_df,id_vars='Algorithm')

performance_df1

sns.catplot(x='Algorithm',y='value',hue='variable',data=performance_df1,
kind='bar',height=5 )
plt.ylim(0.5,1.0)
plt.xticks(rotation='vertical')
plt.show()

"""Model impove
1.Chance the max_featurers parameter of TfIdf
"""

temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)

temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_scaling':accuracy_scores,'Precision_scaling':precision_scores}).sort_values('Precision_scaling',ascending=False)

new_df = performance_df.merge(temp_df,on='Algorithm')

new_df_scaled = new_df.merge(temp_df,on='Algorithm')

temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_num_chars':accuracy_scores,'Precision_num_chars':precision_scores}).sort_values('Precision_num_chars',ascending=False)

new_df_scaled.merge(temp_df,on='Algorithm')

# Voting Classifier
svc = SVC(kernel='sigmoid', gamma=1.0,probability=True)
mnb = MultinomialNB()
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import VotingClassifier

voting = VotingClassifier(estimators=[('svm', svc), ('nb', mnb), ('et', etc)],voting='soft')

voting.fit(x_train,y_train)

y_pred = voting.predict(x_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred))

# Applying stacking
estimators=[('svm', svc), ('nb', mnb), ('et', etc)]
final_estimator=RandomForestClassifier()

from sklearn.ensemble import StackingClassifier

clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)

clf.fit(x_train,y_train)
y_pred = clf.predict(x_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred))

import pickle
pickle.dump(tfidf,open('new_vectorizer.pkl','wb'))
pickle.dump(mnb,open('new_model.pkl','wb'))

"""**Above is the model selection by varifying the the model for the prediction**

**Below**
**this is the chosen model**
and it's pickle file is valid mean use freely there is no error in this

pandas version: 2.0.3
scikit-learn version: 1.2.2
matplotlib version: 3.7.1
seaborn version: 0.13.1
nltk version: 3.8.1
wordcloud version: 1.9.3
xgboost version: 2.0.3
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier

# Assuming 'folder' is your dataframe
# Replace 'folder' with the actual dataframe variable you have
x = tfidf.fit_transform(folder['transform_text']).toarray()

scaler = MinMaxScaler()
x = scaler.fit_transform(x)

y = folder['Check'].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

mnb = MultinomialNB()
mnb.fit(x_train, y_train)

# Save the model and vectorizer
pickle.dump(tfidf, open('vectorizer.pkl', 'wb'))
pickle.dump(mnb, open('model.pkl', 'wb'))

print("Model and vectorizer saved successfully!")